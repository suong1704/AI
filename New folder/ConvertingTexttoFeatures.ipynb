{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"I am learning NLP\"\n",
    "dt = pd.get_dummies(text1.split())\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Using Count Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Phương pháp sử dụng phương thức chuyển đổi văn bản thành feature là một count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cách chuyển đổi văn bản thành vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text2 = [\"I love NLP and I will learn NLP in 2month \"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenizing\n",
    "vectorizer.fit(text2)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text2)\n",
    "# summarize & generating output\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Import textblob\n",
    "# from textblob import TextBlob\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "# from nltk import bigrams\n",
    "\n",
    "# Text = \"I am learning NLP\"\n",
    "# #For unigram : Use n = 1\n",
    "# TextBlob(Text).ngrams(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram-based features for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the function\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Text\n",
    "text = [\"I love NLP and I will learn NLP in 2month \"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "# tokenizing\n",
    "vectorizer.fit(text)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize & generating output\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from nltk import bigrams\n",
    "# import itertools\n",
    "\n",
    "# def co_occurrence_matrix_method(corpus):\n",
    "#    vocab = set(corpus)\n",
    "#    vocab = list(vocab)\n",
    "#    vocab_to_index = { word:i for i, word in enumerate(vocab) }\n",
    "#    # Create bigrams from all words in corpus\n",
    "#    bi_grams = list(bigrams(corpus))\n",
    "#    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
    "#    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "#    # Initialise co-occurrence matrix\n",
    "#    # co_occurrence_matrix[current][previous]\n",
    "#    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "#    # Loop through the bigrams taking the current and previous word,\n",
    "#    # and the number of occurrences of the bigram.\n",
    "#    for bigram in bigram_freq:\n",
    "#       current = bigram[0][1]\n",
    "#       previous = bigram[0][0]\n",
    "#       count = bigram[1]\n",
    "#       pos_current = vocab_to_index[current]\n",
    "#       pos_previous = vocab_to_index[previous]\n",
    "#       co_occurrence_matrix[pos_current][pos_previous] = count\n",
    "#       co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
    "#    # return the matrix and the index\n",
    "#    return co_occurrence_matrix,vocab_to_index\n",
    "# text4 = [\"I love NLP and I will learn NLP in 2month \"]\n",
    "# co_occurrence_matrix,vocab_to_index = co_occurrence_matrix_method(text4)\n",
    "\n",
    "# sentences = [['I', 'love', 'nlp'],\n",
    "#  ['I', 'love','to' 'learn'],\n",
    "#  ['nlp', 'is', 'future'],\n",
    "#  ['nlp', 'is', 'cool']]\n",
    "# # create one list using many lists\n",
    "# merged = list(itertools.chain.from_iterable(sentences))\n",
    "# matrix = co_occurrence_matrix(merged)\n",
    "# # generate the matrix\n",
    "# CoMatrixFinal = pd.DataFrame(matrix[0], index=vocab_to_index, \n",
    "# columns=vocab_to_index)\n",
    "# print(CoMatrixFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hash Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# # list of text documents\n",
    "# text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# # Let’s create the HashingVectorizer of a vector size of 10\n",
    "# # transform\n",
    "# vectorizer = HashingVectorizer(n_features=10)\n",
    "# # create the hashing vector\n",
    "# vector = vectorizer.transform(text)\n",
    "# # summarize the vector\n",
    "# print(vector.shape)\n",
    "# print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# #Import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "# \"The dog.\",\n",
    "# \"The fox\"]\n",
    "\n",
    "# #Create the transform\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# #Tokenize and build vocab\n",
    "# vectorizer.fit(Text)\n",
    "# #Summarize\n",
    "# print(vectorizer.vocabulary_)\n",
    "# print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7 Implementing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Phương pháp này có thể nắm được những vấn đề phức tạp hơn như mỗi quan hệ ngữ nghĩa giữa các từ , đó là tần suất các từ xuất hiện gần nhau\n",
    "- Example:\n",
    "a. I am eating an apple\n",
    "b. I am using apple\n",
    "+ 'apple' đưa ra hai ý nghĩa khác nhau\n",
    "-> Làm thế nào để máy hiểu 'apply' có nghĩ nào\n",
    "- Tạo ra một đại diện cho từ , có thể nắm bắt được ý nghĩa , mối quán hệ ngữ nghĩa và bối cảnh khác nhau khi chúng được sử dụng\n",
    "-> Nhúng từ ( Word Embeddings ): là kỹ thuật học tập tính năng trong đó từ vựng được ánh xạ tới các vectơ của các số thực, dựa vào đó ta có thể nắm bắt hệ thống phân cấp theo ngữ cảnh.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\acer\\Desktop\\AI\\New folder\\ConvertingTexttoFeatures.ipynb Cell 21'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/acer/Desktop/AI/New%20folder/ConvertingTexttoFeatures.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/acer/Desktop/AI/New%20folder/ConvertingTexttoFeatures.ipynb#ch0000016?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/acer/Desktop/AI/New%20folder/ConvertingTexttoFeatures.ipynb#ch0000016?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/__init__.py?line=6'>7</a>\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/__init__.py?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/__init__.py?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/__init__.py?line=13'>14</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m'\u001b[39m\u001b[39mgensim\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/__init__.py?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m logger\u001b[39m.\u001b[39mhandlers:  \u001b[39m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/__init__.py?line=0'>1</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/__init__.py?line=2'>3</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/__init__.py?line=4'>5</a>\u001b[0m \u001b[39m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/__init__.py?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mindexedcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m IndexedCorpus  \u001b[39m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/__init__.py?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmmcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m MmCorpus  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/__init__.py?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbleicorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m BleiCorpus  \u001b[39m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/indexedcorpus.py?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/indexedcorpus.py?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/indexedcorpus.py?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/indexedcorpus.py?line=15'>16</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/corpora/indexedcorpus.py?line=18'>19</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mIndexedCorpus\u001b[39;00m(interfaces\u001b[39m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/interfaces.py?line=6'>7</a>\u001b[0m \u001b[39m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/interfaces.py?line=7'>8</a>\u001b[0m \n\u001b[0;32m      <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/interfaces.py?line=8'>9</a>\u001b[0m \u001b[39mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/interfaces.py?line=13'>14</a>\u001b[0m \n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/interfaces.py?line=14'>15</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/interfaces.py?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/interfaces.py?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/interfaces.py?line=21'>22</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/interfaces.py?line=24'>25</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCorpusABC\u001b[39;00m(utils\u001b[39m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\matutils.py:1031\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/matutils.py?line=1025'>1026</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1.\u001b[39m \u001b[39m-\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mlen\u001b[39m(set1 \u001b[39m&\u001b[39m set2)) \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(union_cardinality)\n\u001b[0;32m   <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/matutils.py?line=1028'>1029</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/matutils.py?line=1029'>1030</a>\u001b[0m     \u001b[39m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/matutils.py?line=1030'>1031</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_matutils\u001b[39;00m \u001b[39mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[0;32m   <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/matutils.py?line=1032'>1033</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/acer/AppData/Local/Programs/Python/Python310/lib/site-packages/gensim/matutils.py?line=1033'>1034</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mlogsumexp\u001b[39m(x):\n",
      "File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "Text = \"I love NLP and I will learn NLP in 2 months\"\n",
    "sentences = [['I', 'love', 'nlp'],\n",
    " ['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    " ['nlp', 'is', 'future'],\n",
    " ['nlp', 'saves', 'time', 'and', 'solves', \n",
    "'lot', 'of', 'industry', 'problems'],\n",
    " ['nlp', 'uses', 'machine', 'learning']]\n",
    "# training the model\n",
    "skipgram = Word2Vec(sentences, size =50, window = 3, min_count=1,\n",
    "sg = 1)\n",
    "print(skipgram)\n",
    "# access vector for one word\n",
    "print(skipgram['nlp'])\n",
    "# access vector for another one word\n",
    "print(skipgram['deep'])\n",
    "# save model\n",
    "skipgram.save('skipgram.bin')\n",
    "# load model\n",
    "skipgram = Word2Vec.load('skipgram.bin')\n",
    "\n",
    "# T – SNE plot\n",
    "X = skipgram[skipgram.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(skipgram.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram\n",
    "- Được sử dụng để dự đoán xác suất một từ được đưa ra trong bối bảnh 1 từ hoặc nhiều từ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "import gensim\n",
    "\n",
    "#Example sentences\n",
    "sentences = [['I', 'love', 'nlp'],\n",
    " ['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    " ['nlp', 'is', 'future'],\n",
    " ['nlp', 'saves', 'time', 'and', 'solves', \n",
    "'lot', 'of', 'industry', 'problems'],\n",
    " ['nlp', 'uses', 'machine', 'learning']]\n",
    "\n",
    "# training the model\n",
    "cbow = Word2Vec(sentences, size =50, window = 3, min_count=1,sg = 1)\n",
    "print(cbow)\n",
    "# access vector for one word\n",
    "print(cbow['nlp'])\n",
    "# save model\n",
    "cbow.save('cbow.bin')\n",
    "# load model\n",
    "cbow = Word2Vec.load('cbow.bin')\n",
    "# T – SNE plot\n",
    "X = cbow[cbow.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(cbow.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()\n",
    "\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('C:\\\\Users\\\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#Checking how similarity works.\n",
    "print (model.similarity('this', 'is'))\n",
    "print (model.similarity('post', 'book'))\n",
    "# Finding the odd one out.\n",
    "model.doesnt_match('breakfast cereal dinner lunch'.split())\n",
    "# It is also finding the relations between words.\n",
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "#Example sentences\n",
    "sentences = [['I', 'love', 'nlp'],\n",
    " ['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    " ['nlp', 'is', 'future'],\n",
    " ['nlp', 'saves', 'time', 'and', 'solves', \n",
    "'lot', 'of', 'industry', 'problems'],\n",
    " ['nlp', 'uses', 'machine', 'learning']]\n",
    "fast = FastText(sentences,size=20, window=1, min_count=1, \n",
    "workers=5, min_n=1, max_n=2)\n",
    "# vector for word nlp\n",
    "print(fast['nlp'])\n",
    "print(fast['deep'])\n",
    "\n",
    "# load model\n",
    "fast = Word2Vec.load('fast.bin')\n",
    "# visualize\n",
    "X = fast[fast.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(fast.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "229cdfb8eedfa4964725b7eb0da8d7a63b25d97a6ab808f09bd6b506844c0629"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
